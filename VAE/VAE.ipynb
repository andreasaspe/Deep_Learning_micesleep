{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsaNbwpxjSqz"
   },
   "source": [
    "# ***Notebook for creating VAE***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwrqusZIlT2Z"
   },
   "source": [
    "## Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jvnLtKhlWm9",
    "outputId": "eaa7637b-ea2b-4241-9420-d9cdda999773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: umap-learn in /Users/andreasaspe/opt/anaconda3/envs/new_env/lib/python3.9/site-packages (0.5.3)\n",
      "Requirement already satisfied: tqdm in /Users/andreasaspe/opt/anaconda3/envs/new_env/lib/python3.9/site-packages (from umap-learn) (4.64.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/andreasaspe/opt/anaconda3/envs/new_env/lib/python3.9/site-packages (from umap-learn) (0.5.8)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/andreasaspe/opt/anaconda3/envs/new_env/lib/python3.9/site-packages (from umap-learn) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /Users/andreasaspe/opt/anaconda3/envs/new_env/lib/python3.9/site-packages (from umap-learn) (1.0.2)\n",
      "Requirement already satisfied: numba>=0.49 in /Users/andreasaspe/opt/anaconda3/envs/new_env/lib/python3.9/site-packages (from umap-learn) (0.55.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/andreasaspe/opt/anaconda3/envs/new_env/lib/python3.9/site-packages (from umap-learn) (1.21.5)\n",
      "Requirement already satisfied: setuptools in /Users/andreasaspe/opt/anaconda3/envs/new_env/lib/python3.9/site-packages (from numba>=0.49->umap-learn) (65.6.3)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /Users/andreasaspe/opt/anaconda3/envs/new_env/lib/python3.9/site-packages (from numba>=0.49->umap-learn) (0.38.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/andreasaspe/opt/anaconda3/envs/new_env/lib/python3.9/site-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/andreasaspe/opt/anaconda3/envs/new_env/lib/python3.9/site-packages (from scikit-learn>=0.22->umap-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set_style(\"whitegrid\")\n",
    "import math \n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import Distribution\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # setting ignore as a parameter and further adding category\n",
    "#from google.colab import files # UNCOMMENT IF YOU ARE USING GOOGLE DRIVE\n",
    "from matplotlib.colors import ListedColormap\n",
    "#For plotting\n",
    "from IPython.display import Image, display, clear_output\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.distributions import Normal\n",
    "!pip install umap-learn\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLLRm3qkjhVV"
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8RQCayUksmt"
   },
   "source": [
    "Connecting to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k0lViAVIkD0X",
    "outputId": "dff3b5ab-5834-4cbb-b225-729cbb99f8d6"
   },
   "outputs": [],
   "source": [
    "# UNCOMMENT IF YOU ARE USING GOOGLE DRIVE\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6SNH5bJlACv"
   },
   "source": [
    "Defining path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ayNr2JtikL0C"
   },
   "outputs": [],
   "source": [
    "# UNCOMMENT IF YOU ARE USING GOOGLE DRIVE\n",
    "#drive_path = 'drive/My Drive/Colab Notebooks/Deep_learning_02456/Project/mouseSleepAnalysis-master/datasetsPaper/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or via local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_path = 'preprocessed_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THDH8MzUlB1l"
   },
   "source": [
    "Loading and treating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kr6wDa3bjkxG",
    "outputId": "b1b74b02-3064-4217-87b7-c948bfbba699"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fk/q49x7w9j6t53t4bvkbj_nkdm0000gp/T/ipykernel_2204/2543215160.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  data_sleepstage = dLoad[\"epochsLinked\"].astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "file_name = \"experiment1.npz\"\n",
    "dLoad = np.load(drive_path+file_name)\n",
    "\n",
    "data = dLoad[\"d\"].astype(np.float32)\n",
    "data_sleepstage = dLoad[\"epochsLinked\"].astype(np.int)\n",
    "data_sleepstage = data_sleepstage[:,2] #Extracting the sleep stages\n",
    "obskeys   = dLoad['epochsLinked'].astype(np.float32)\n",
    "epochTime = dLoad['epochTime']\n",
    "\n",
    "data = torch.from_numpy(data) #Convert data to tensor\n",
    "data_sleepstage = torch.from_numpy(data_sleepstage) #Convert data to tensor\n",
    "\n",
    "no_features = data[0].shape #Det er 11 - data er 11-dimensionelt. 10 kombinationer af bands og 1 EMG.\n",
    "no_obs = data.shape[0]\n",
    "\n",
    "#Define batch_size\n",
    "batch_size = 256\n",
    "test_batch_size = 256*4\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,\n",
    "                                                    data_sleepstage,\n",
    "                                                    test_size=0.2,\n",
    "                                                    )\n",
    "no_obs_train = X_train.shape[0]\n",
    "no_obs_test = X_test.shape[0]\n",
    "\n",
    "\n",
    "#index 1 i SS kolonne: Wakefulness\n",
    "#index 2 i SS kolonne: NREM\n",
    "#index 3 i SS kolonne: REM\n",
    "\n",
    "# 1)  d (Nx11), N = # of epochs and columns refer to the possible ratios, EMG is the last column \n",
    "# 2)  epochsLinked (Nx4), N = number of epochs, columns are\n",
    "#       a) Column 1 => epoch ID \n",
    "#       b) Column 2=>  epoch index \n",
    "#       c) Column 3=> SS \n",
    "#       d) Column 4 => subject ID \n",
    "# 3)  epochTime \n",
    "#\t\t\t\ta) (Nx3), N = number of epochs and columns: \n",
    "#\t\t\t\t\t\ti) Column 1 => epoch ID\n",
    "#\t\t\t\t\t\tii) Column 2 => recording mode\n",
    "#\t\t\t\t\t\tiii) Column 3 => epoch date-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBDYuC95kyvs"
   },
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g75D4xGelkaz"
   },
   "source": [
    "### Defining reparameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TItgJ392lnVh"
   },
   "outputs": [],
   "source": [
    "class ReparameterizedDiagonalGaussian(Distribution):\n",
    "    \"\"\"\n",
    "    A distribution `N(y | mu, sigma I)` compatible with the reparameterization trick given `epsilon ~ N(0, 1)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu: Tensor, log_sigma:Tensor):\n",
    "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
    "        self.mu = mu\n",
    "        self.sigma = log_sigma.exp()\n",
    "        \n",
    "    def sample_epsilon(self) -> Tensor:\n",
    "        \"\"\"`\\eps  ~ N(0, I)`\"\"\"\n",
    "        return torch.empty_like(self.mu).normal_()\n",
    "        \n",
    "    def sample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (without gradients)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.rsample()\n",
    "        \n",
    "    def rsample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
    "        dist = torch.distributions.Normal(self.mu, self.sigma)\n",
    "        return dist.rsample()\n",
    "        \n",
    "    def log_prob(self, z:Tensor) -> Tensor:\n",
    "        \"\"\"return the log probability: log `p(z)`\"\"\"\n",
    "        dist = torch.distributions.Normal(self.mu, self.sigma)\n",
    "        return dist.log_prob(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEKFHZDnsmSm"
   },
   "source": [
    "### VAE network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9Ubn6z6splc",
    "outputId": "ce370697-86e1-4b19-e608-a6bf2b319f33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=11, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=11, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=6, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=11, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=11, out_features=11, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    \"\"\"A Variational Autoencoder with\n",
    "    * a Bernoulli observation model `p_\\theta(x | z) = B(x | g_\\theta(z))`\n",
    "    * a Gaussian prior `p(z) = N(z | 0, I)`\n",
    "    * a Gaussian posterior `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape:torch.Size, latent_features:int) -> None:\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.latent_features = latent_features\n",
    "        self.observation_features = input_shape[0]\n",
    "        \n",
    "\n",
    "        # Inference Network\n",
    "        # Encode the observation `x` into the parameters of the posterior distribution\n",
    "        # `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x)), \\mu(x),\\log\\sigma(x) = h_\\phi(x)`\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=self.observation_features, out_features=11),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=11, out_features=8),\n",
    "            nn.ReLU(),\n",
    "            # A Gaussian is fully characterised by its mean \\mu and variance \\sigma**2\n",
    "            nn.Linear(in_features=8, out_features=2*latent_features) # <- note the 2*latent_features\n",
    "        )\n",
    "        \n",
    "        # Generative Model\n",
    "        # Decode the latent sample `z` into the parameters of the observation model\n",
    "        # `p_\\theta(x | z) = \\prod_i B(x_i | g_\\theta(x))`\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_features, out_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=8, out_features=11),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=11, out_features=self.observation_features)\n",
    "        )\n",
    "        \n",
    "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
    "        self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features])))\n",
    "        self.log_sigma = torch.nn.Parameter(torch.tensor([0.]))\n",
    "        #self.sigma = torch.tensor([1.])\n",
    "        \n",
    "    def posterior(self, x:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\"\"\"\n",
    "        \n",
    "        # compute the parameters of the posterior\n",
    "        h_x = self.encoder(x)\n",
    "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
    "        \n",
    "        # return a distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def prior(self, batch_size:int=1)-> Distribution:\n",
    "        \"\"\"return the distribution `p(z)`\"\"\"\n",
    "        prior_params = self.prior_params.expand(batch_size, *self.prior_params.shape[-1:])\n",
    "        mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
    "        \n",
    "        # return the distribution `p(z)`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def observation_model(self, z:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
    "        f_z = self.decoder(z)\n",
    "        mu, = f_z.chunk(1, dim=-1)\n",
    "        log_sigma = self.log_sigma.expand(mu.shape[0],mu.shape[1])\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma) #Normal(mu, self.log_sigma.exp())\n",
    "        \n",
    "\n",
    "    def forward(self, x) -> Dict[str, Any]:\n",
    "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
    "        \n",
    "        # flatten the input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # define the posterior q(z|x) / encode x into q(z|x)\n",
    "        qz = self.posterior(x)\n",
    "        \n",
    "        # define the prior p(z)\n",
    "        pz = self.prior(batch_size=x.size(0))\n",
    "        \n",
    "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
    "        z = qz.rsample()\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'qz': qz, 'z': z}\n",
    "    \n",
    "    \n",
    "    def sample_from_prior(self, batch_size:int=100):\n",
    "        \"\"\"sample z~p(z) and return p(x|z)\"\"\"\n",
    "        \n",
    "        # degine the prior p(z)\n",
    "        pz = self.prior(batch_size=batch_size)\n",
    "        \n",
    "        # sample the prior \n",
    "        z = pz.rsample()\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'z': z}\n",
    "\n",
    "\n",
    "latent_features = 3\n",
    "vae = VariationalAutoencoder(no_features, latent_features)\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RjJiKH8yDHK"
   },
   "source": [
    "### VAE inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OLbaThlfyFjq"
   },
   "outputs": [],
   "source": [
    "def reduce(x:Tensor) -> Tensor:\n",
    "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "    return x.view(x.size(0), -1).sum(dim=1)\n",
    "\n",
    "class VariationalInference(nn.Module):\n",
    "    def __init__(self, beta:float=1.):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, model:nn.Module, x:Tensor) -> Tuple[Tensor, Dict]:\n",
    "        \n",
    "        # forward pass through the model\n",
    "        outputs = model(x)\n",
    "        \n",
    "        # unpack outputs\n",
    "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "        \n",
    "        # evaluate log probabilities\n",
    "        log_px = reduce(px.log_prob(x))\n",
    "        log_pz = reduce(pz.log_prob(z))\n",
    "        log_qz = reduce(qz.log_prob(z))\n",
    "        \n",
    "        # compute the ELBO with and without the beta parameter: \n",
    "        # `L^\\beta = E_q [ log p(x|z) ] - \\beta * D_KL(q(z|x) | p(z))`\n",
    "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)`\n",
    "        kl = log_qz - log_pz\n",
    "        elbo = log_px - kl # <- your code here\n",
    "        beta_elbo = log_px - self.beta*kl # <- your code here\n",
    "        \n",
    "        # loss\n",
    "        loss = -beta_elbo.mean()\n",
    "        \n",
    "        # prepare the output\n",
    "        with torch.no_grad():\n",
    "            diagnostics = {'elbo': beta_elbo, 'log_px':log_px, 'kl': kl, 'log_qz': log_qz, 'log_pz':log_pz}\n",
    "            \n",
    "        return loss, diagnostics, outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFfLGyncstLF"
   },
   "source": [
    "Dummy pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhIu-bFJympn",
    "outputId": "8adcc080-e559-4a12-ff1a-8d8e9ff15346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss   | mean =     15.619, shape: []\n",
      "elbo   | mean =    -15.619, shape: [256]\n",
      "log_px | mean =    -15.252, shape: [256]\n",
      "kl     | mean =      0.367, shape: [256]\n",
      "log_qz | mean =     -4.613, shape: [256]\n",
      "log_pz | mean =     -4.980, shape: [256]\n"
     ]
    }
   ],
   "source": [
    "vi = VariationalInference(beta=1.0)\n",
    "loss, diagnostics, outputs = vi(vae, data[0:batch_size])\n",
    "print(f\"{'loss':6} | mean = {loss:10.3f}, shape: {list(loss.shape)}\")\n",
    "for key, tensor in diagnostics.items():\n",
    "    print(f\"{key:6} | mean = {tensor.mean():10.3f}, shape: {list(tensor.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G53LwrRy7k-p"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea9E9yjW7jyV"
   },
   "source": [
    "#### Define a plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "FiIQ3PgH7wJp"
   },
   "outputs": [],
   "source": [
    "#Define font\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 22}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "\n",
    "def plot_2d_latents(ax,qz,z,y):\n",
    "    z = z.to('cpu')\n",
    "    y = y.to('cpu')\n",
    "    scale_factor = 2\n",
    "    batch_size = z.shape[0]\n",
    "    palette = sns.color_palette()\n",
    "\n",
    "    labels = ['Wakefulness','NREM','REM']\n",
    "\n",
    "    colors = ListedColormap(['green', 'orange', 'red'])\n",
    "\n",
    "    scatter = ax.scatter(z[:, 0], z[:, 1], cmap=colors, c=y)\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=labels)\n",
    "\n",
    "    ax.set_xlim([-3, 3])\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "def plot_latents(ax, z, y, perplexity=30, early_exaggeration = 12):\n",
    "    z = z.to('cpu')\n",
    "    palette = sns.color_palette()\n",
    "    colors = [palette[l] for l in y]\n",
    "    z = TSNE(n_components=2,perplexity=perplexity,early_exaggeration=early_exaggeration).fit_transform(z)\n",
    "    labels = ['Wakefulness','NREM','REM']\n",
    "    colors = ListedColormap(['green', 'orange', 'red'])\n",
    "    scatter = ax.scatter(z[:, 0], z[:, 1], cmap=colors, c=y)\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=labels, markerscale=2, loc='upper left')\n",
    "    ax.grid(False)\n",
    "\n",
    "\n",
    "def make_vae_plots(vae, x, y, outputs, training_data, validation_data, epoch, tmp_img=\"tmp_vae_out.png\", figsize=(18, 18)): #figsize=(18, 18)\n",
    "  fig, axes = plt.subplots(2, 2, figsize=figsize, squeeze=False)\n",
    "\n",
    "  ax = axes[0, 0]\n",
    "  ax.set_title(r'ELBO: $\\mathcal{L} ( \\mathbf{x} )$')\n",
    "  ax.set_xlabel(r'Epochs')\n",
    "  ax.set_ylabel(r'$\\mathcal{L} ( \\mathbf{x} )$')\n",
    "  ax.plot(training_data['elbo'], label='Training',linewidth=8)\n",
    "  ax.plot(validation_data['elbo'], label='Validation',linewidth=8)\n",
    "  ax.legend(loc='upper left')\n",
    "  ax.grid(False)\n",
    "\n",
    "  # plot the latent samples\n",
    "  try:\n",
    "      z = outputs['z']\n",
    "      if z.shape[1] == 2:\n",
    "          axes[0,1].set_title(r'Latent Samples $\\mathbf{z} \\sim q(\\mathbf{z} | \\mathbf{x})$')\n",
    "          qz = outputs['qz']\n",
    "          plot_2d_latents(axes[0, 1], qz, z, y)\n",
    "      else:\n",
    "          axes[0, 1].set_title(r'Latent Samples $\\mathbf{z} \\sim q(\\mathbf{z} | \\mathbf{x})$ (t-SNE)')\n",
    "          plot_latents(axes[0, 1], z, y)\n",
    "  except Exception as e:\n",
    "      print(f\"Could not generate the plot of the latent samples because of exception\")\n",
    "      print(e)\n",
    "\n",
    "  #Input heatmap\n",
    "  ax = axes[1, 0]\n",
    "  ax.set_title(r'Input data')\n",
    "  s1 = sns.heatmap(x[:100,:], ax = ax)\n",
    "  s1.set_xlabel('Features')\n",
    "  s1.set_ylabel('Batch')\n",
    "  \n",
    "  #Reconstruction heatmap\n",
    "  ax = axes[1, 1]\n",
    "  ax.set_title(r'Reconstruction $\\mathbf{x} \\sim p(\\mathbf{x} | \\mathbf{z}), \\mathbf{z} \\sim q(\\mathbf{z} | \\mathbf{x})$')\n",
    "  px = outputs['px']\n",
    "  x_sample = px.sample().to('cpu')\n",
    "  s2 = sns.heatmap(x_sample[:100,:], ax = ax)\n",
    "  s2.set_xlabel('Features')\n",
    "  s2.set_ylabel('Batch')\n",
    "\n",
    "  # display\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(tmp_img)\n",
    "  #folder_name = \"hand_in\" #Name of folder in which the plots for every epoch should be saved # UNCOMMENT IF YOU ARE USING GOOGLE DRIVE\n",
    "  #new_path = 'drive/My Drive/Colab Notebooks/Deep_learning_02456/Project/Plots/'+folder_name+'/epoch'+str(epoch) #Definition of full destitation path # UNCOMMENT IF YOU ARE USING GOOGLE DRIVE\n",
    "  #plt.savefig(new_path) # UNCOMMENT IF YOU ARE USING GOOGLE DRIVE\n",
    "  plt.close(fig)\n",
    "  display(Image(filename=tmp_img))\n",
    "  clear_output(wait=True)\n",
    "\n",
    "  os.remove(tmp_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFqzS1pVW1OR"
   },
   "source": [
    "#### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4An-QoamW4_r",
    "outputId": "fdc487f6-c493-4381-9990-e809229164e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# VAE\n",
    "latent_features = 3\n",
    "vae = VariationalAutoencoder(no_features, latent_features)\n",
    "\n",
    "# Evaluator: Variational Inference\n",
    "beta = 1\n",
    "vi = VariationalInference(beta=beta)\n",
    "\n",
    "# The Adam optimizer works really well with VAEs.\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "# define dictionary to store the training curves\n",
    "training_data = defaultdict(list)\n",
    "validation_data = defaultdict(list)\n",
    "training_data_test = defaultdict(list) #MY TESTINGS\n",
    "\n",
    "epoch = 0\n",
    "num_epochs = math.floor(no_obs_test/test_batch_size) #Max number of epochs (because of testing) is math.floor(no_obs_test/test_batch_size)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "# move the model to the device\n",
    "vae = vae.to(device)\n",
    "\n",
    "#Number of iterations in each epoch\n",
    "no_iter = math.ceil(no_obs_train/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZN4_akL7qRa"
   },
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XE45tvOU7vKL",
    "outputId": "4525b3c1-c7bd-4910-fe73-f72e03ad6bc5"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fk/q49x7w9j6t53t4bvkbj_nkdm0000gp/T/ipykernel_2204/94904587.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# gather data for the current bach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             adam(params_with_grad,\n\u001b[0m\u001b[1;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training..\n",
    "while epoch < num_epochs:\n",
    "    #print(\"Epoch nr.: \"+str(epoch))\n",
    "    training_epoch_data = defaultdict(list)\n",
    "    vae.train()\n",
    "    \n",
    "    for i in range(no_iter):\n",
    "        #if i%1000 == 0:\n",
    "          #print(\"   Batch nr.: \"+str(i) + \" out of \"+ str(no_iter-1))\n",
    "        x = X_train[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss, diagnostics, outputs = vi(vae, x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # gather data for the current bach\n",
    "        for k, v in diagnostics.items():\n",
    "            training_epoch_data[k] += [v.mean().item()]\n",
    "\n",
    "    # gather data for the full epoch\n",
    "    for k, v in training_epoch_data.items():\n",
    "        training_data[k] += [np.mean(training_epoch_data[k])]\n",
    "        training_data_test[epoch,k] += [np.mean(training_epoch_data[k])]\n",
    "\n",
    "    # Evaluate on a single batch, do not propagate gradients\n",
    "    with torch.no_grad():\n",
    "        vae.eval()\n",
    "        \n",
    "        # Just load a single batch from the test loader\n",
    "        x = X_test[epoch*test_batch_size:(epoch+1)*test_batch_size]\n",
    "        y = y_test[epoch*test_batch_size:(epoch+1)*test_batch_size]\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss, diagnostics, outputs = vi(vae, x)\n",
    "        \n",
    "        # gather data for the validation step\n",
    "        for k, v in diagnostics.items():\n",
    "            validation_data[k] += [v.mean().item()]\n",
    "    \n",
    "    make_vae_plots(vae, x, y, outputs, training_data, validation_data, epoch)\n",
    "\n",
    "    epoch+= 1\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "qwrqusZIlT2Z",
    "trY4zMmtuFek",
    "rNe5M3rJKym-",
    "qxXQpFaQuKdT"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9df1babbdc12089a35dd551870a99fe78f907c3a84c396e2be8356d102c548c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
